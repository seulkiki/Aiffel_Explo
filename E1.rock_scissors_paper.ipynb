{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressed-finnish",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-boxing",
   "metadata": {},
   "source": [
    "###  딥러닝 단계: 데이터 준비 → 딥러닝 네트워크 설계 → 학습 → 테스트(평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-reynolds",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-confidentiality",
   "metadata": {},
   "source": [
    "- 학습용 데이터: (x_train, y_train) ,  시험용 데이터: (x_test, y_test)\n",
    "- 데이터 전처리: \n",
    "\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "print('최소값:',np.min(x_train_norm), ' 최대값:',np.max(x_train_norm))\n",
    "\n",
    "*각 픽셀의 값이 0~255 사이 범위에 있으므로 데이터들을 255.0 으로 나누어주어 최소값이 0, 최대값이 1에 근접하도록 나오는지 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-visitor",
   "metadata": {},
   "source": [
    "### 2. 데이터 네트워크 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-belief",
   "metadata": {},
   "source": [
    "- Sequential Model 사용하기\n",
    "- Conv2D레어의 첫 번째 인자는 사용한 이미지의 수\n",
    "- Dense레이어의 첫 번째 인자: 분류기에 사용되는 뉴런의 숫자(복잡할 수록 수를 늘리기)\n",
    "- Dense레이어의 class \n",
    "- 모델확인하기: model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-nickname",
   "metadata": {},
   "source": [
    "### 3. 데이터 네트워크 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-tampa",
   "metadata": {},
   "source": [
    "\n",
    "- 학습용 데이터(x_train)\n",
    "- 네트워크의 입력은 (데이터갯수, 이미지 크기 x, 이미지 크기 y, 채널수) 와 같은 형태이므로 ( , , , ) 형태로 reshape하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-modeling",
   "metadata": {},
   "source": [
    "### 4.데이터 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-baptist",
   "metadata": {},
   "source": [
    "- 시험용 데이터(x_test)\n",
    "- model.predict()를 사용하면 model이 입력값을 보고 실제로 추론한 확률분포를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-civilization",
   "metadata": {},
   "source": [
    "### 5.수정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-regular",
   "metadata": {},
   "source": [
    "- Conv2D 레이어에서 입력 이미지의 특정 수를 늘리기, 줄이기\n",
    "- Dense 레이어에서 뉴런수 수정\n",
    "- epoch(학습 반복 횟수) 변경하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-winning",
   "metadata": {},
   "source": [
    "## *가위바위보 분류*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-louisville",
   "metadata": {},
   "source": [
    "### 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exotic-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-jacksonville",
   "metadata": {},
   "source": [
    "### 28X28 사이즈로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intended-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpful-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coordinate-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103  images to be resized.\n",
      "103  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-prefix",
   "metadata": {},
   "source": [
    "### 학습데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "square-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 303 입니다.\n",
      "x_train shape: (303, 28, 28, 3)\n",
      "y_train shape: (303,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=303):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-gross",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 설계하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "homeless-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 28,627\n",
      "Trainable params: 28,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "#Dense: 분류가 얼마나 복잡할 것인가 복잡한 문제일 수록 숫자를 늘리기\n",
    "\n",
    "    \n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "german-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 5s 192ms/step - loss: 1.1077 - accuracy: 0.3083\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0976 - accuracy: 0.3449\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0830 - accuracy: 0.3833\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0726 - accuracy: 0.3693\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0592 - accuracy: 0.4834\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0576 - accuracy: 0.4716\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0417 - accuracy: 0.4542\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0071 - accuracy: 0.6252\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9907 - accuracy: 0.5038\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9522 - accuracy: 0.6021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f24cc40a750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-earthquake",
   "metadata": {},
   "source": [
    "#### 정확도 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-astronomy",
   "metadata": {},
   "source": [
    "### Test 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-treasurer",
   "metadata": {},
   "source": [
    "#### Test data Resize하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "square-grammar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n"
     ]
    }
   ],
   "source": [
    "# 이미지 크기 조절 함수 정의\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img) # image.open : 이미지 불러오기\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "        # ANTIALIAS : 높은 해상도의 사진 또는 영상을 낮은 해상도로 변환하거나 나타낼때\n",
    "        \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "#TEST 자료    \n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들이자\n",
    "image_dir_path_test = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path_test)\n",
    "\n",
    "# 바위 리사이즈\n",
    "image_dir_path_test = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path_test)\n",
    "\n",
    "# 보 리사이즈\n",
    "image_dir_path_test = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-enhancement",
   "metadata": {},
   "source": [
    "#### test data 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lasting-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 읽어올 수 있는 load_data() 함수 정의\n",
    "\n",
    "def load_data(img_path, number_of_data):\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endangered-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path, 300)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-breed",
   "metadata": {},
   "source": [
    "### test_accuracy 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grateful-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_channel_3 = 128\n",
    "n_dense=512\n",
    "\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "artistic-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 3)  \n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "environmental-weapon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 160,835\n",
      "Trainable params: 160,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "11/11 [==============================] - 8s 447ms/step - loss: 1.2947 - accuracy: 0.3115 - val_loss: 1.2578 - val_accuracy: 0.3333\n",
      "Epoch 2/25\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.2476 - accuracy: 0.3299 - val_loss: 1.2199 - val_accuracy: 0.3333\n",
      "Epoch 3/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.2040 - accuracy: 0.3365 - val_loss: 1.1919 - val_accuracy: 0.3333\n",
      "Epoch 4/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.1637 - accuracy: 0.3550 - val_loss: 1.2149 - val_accuracy: 0.3333\n",
      "Epoch 5/25\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.0916 - accuracy: 0.4507 - val_loss: 1.1611 - val_accuracy: 0.3367\n",
      "Epoch 6/25\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.9483 - accuracy: 0.6363 - val_loss: 1.4985 - val_accuracy: 0.3333\n",
      "Epoch 7/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.6936 - accuracy: 0.7413 - val_loss: 2.6750 - val_accuracy: 0.2967\n",
      "Epoch 8/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.6364 - accuracy: 0.7568 - val_loss: 1.9547 - val_accuracy: 0.2933\n",
      "Epoch 9/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5591 - accuracy: 0.8055 - val_loss: 2.3122 - val_accuracy: 0.3033\n",
      "Epoch 10/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4115 - accuracy: 0.8540 - val_loss: 2.6952 - val_accuracy: 0.3367\n",
      "Epoch 11/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3549 - accuracy: 0.8612 - val_loss: 2.7919 - val_accuracy: 0.3333\n",
      "Epoch 12/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3618 - accuracy: 0.8830 - val_loss: 2.3926 - val_accuracy: 0.3467\n",
      "Epoch 13/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2947 - accuracy: 0.9150 - val_loss: 2.8501 - val_accuracy: 0.3333\n",
      "Epoch 14/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2937 - accuracy: 0.9103 - val_loss: 3.8289 - val_accuracy: 0.3400\n",
      "Epoch 15/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1980 - accuracy: 0.9569 - val_loss: 3.0302 - val_accuracy: 0.3333\n",
      "Epoch 16/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2168 - accuracy: 0.9402 - val_loss: 3.0671 - val_accuracy: 0.2967\n",
      "Epoch 17/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1919 - accuracy: 0.9473 - val_loss: 3.2225 - val_accuracy: 0.3367\n",
      "Epoch 18/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1629 - accuracy: 0.9661 - val_loss: 3.8414 - val_accuracy: 0.3267\n",
      "Epoch 19/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1623 - accuracy: 0.9668 - val_loss: 4.1591 - val_accuracy: 0.3267\n",
      "Epoch 20/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1532 - accuracy: 0.9666 - val_loss: 4.0231 - val_accuracy: 0.3367\n",
      "Epoch 21/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1432 - accuracy: 0.9728 - val_loss: 5.1491 - val_accuracy: 0.3400\n",
      "Epoch 22/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1245 - accuracy: 0.9752 - val_loss: 5.0188 - val_accuracy: 0.3367\n",
      "Epoch 23/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1196 - accuracy: 0.9870 - val_loss: 4.2648 - val_accuracy: 0.3367\n",
      "Epoch 24/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1038 - accuracy: 0.9847 - val_loss: 3.7134 - val_accuracy: 0.3500\n",
      "Epoch 25/25\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1086 - accuracy: 0.9771 - val_loss: 3.9303 - val_accuracy: 0.3633\n",
      "10/10 - 0s - loss: 3.9303 - accuracy: 0.3633\n",
      "test_loss: 3.9302966594696045 \n",
      "test_accuracy: 0.3633333444595337\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', \n",
    "                              input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "model.add(keras.layers.MaxPooling2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, kernel_regularizer=keras.regularizers.l2(0.001),activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train, batch_size=28,epochs=25, \n",
    "                    verbose=1, validation_data=(x_test_reshaped, y_test),\n",
    "                    callbacks =[callback])\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
